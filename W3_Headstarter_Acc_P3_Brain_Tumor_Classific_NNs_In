{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 3 Headstarter Accelerator Project 3: Brain Tumor Classification with Neural Networks","metadata":{}},{"cell_type":"markdown","source":"Week 3 Headstarter Accelerator Project 3: Brain Tumor Classification with Neural Networks\r\n\r\nIn this project, you will learn how to train neural networks to classify tumors in brain MRI scans. You will learn about how to construct different neural network architectures through transfer learning and custom convolutional layers, and use the Gemini 1.5 Flash model to generate explanations for the model's predictions.","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Training and Evaluation","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/content/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:14.763925Z","iopub.execute_input":"2024-11-14T20:54:14.764332Z","iopub.status.idle":"2024-11-14T20:54:15.862375Z","shell.execute_reply.started":"2024-11-14T20:54:14.764292Z","shell.execute_reply":"2024-11-14T20:54:15.861132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install groq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:15.864516Z","iopub.execute_input":"2024-11-14T20:54:15.864838Z","iopub.status.idle":"2024-11-14T20:54:29.465188Z","shell.execute_reply.started":"2024-11-14T20:54:15.864803Z","shell.execute_reply":"2024-11-14T20:54:29.464057Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:29.466611Z","iopub.execute_input":"2024-11-14T20:54:29.466933Z","iopub.status.idle":"2024-11-14T20:54:30.676455Z","shell.execute_reply.started":"2024-11-14T20:54:29.466888Z","shell.execute_reply":"2024-11-14T20:54:30.675373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_class_paths(path):\n    classes = []\n    class_paths = []\n\n    # Iterate through directories in the training path\n    for label in os.listdir(path):\n        label_path = os.path.join(path, label)\n\n        # Check if it's a directory\n        if os.path.isdir(label_path):\n\n            # Iterate through images in the label directory\n            for image in os.listdir(label_path):\n                image_path = os.path.join(label_path, image)\n\n                # Add class and path to respective lists\n                classes.append(label)\n                class_paths.append(image_path)\n\n    # Create a DataFrame with the collected data\n    df = pd.DataFrame({\n        'Class Path': class_paths,\n        'Class': classes\n    })\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:30.678795Z","iopub.execute_input":"2024-11-14T20:54:30.679250Z","iopub.status.idle":"2024-11-14T20:54:30.686206Z","shell.execute_reply.started":"2024-11-14T20:54:30.679210Z","shell.execute_reply":"2024-11-14T20:54:30.685228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_df = get_class_paths(\"/kaggle/input/brain-tumor-mri-dataset/Training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:30.687556Z","iopub.execute_input":"2024-11-14T20:54:30.687954Z","iopub.status.idle":"2024-11-14T20:54:31.337434Z","shell.execute_reply.started":"2024-11-14T20:54:30.687909Z","shell.execute_reply":"2024-11-14T20:54:31.336343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:31.338780Z","iopub.execute_input":"2024-11-14T20:54:31.339092Z","iopub.status.idle":"2024-11-14T20:54:31.359167Z","shell.execute_reply.started":"2024-11-14T20:54:31.339059Z","shell.execute_reply":"2024-11-14T20:54:31.358241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_df = get_class_paths(\"/kaggle/input/brain-tumor-mri-dataset/Testing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:31.360387Z","iopub.execute_input":"2024-11-14T20:54:31.360744Z","iopub.status.idle":"2024-11-14T20:54:31.595043Z","shell.execute_reply.started":"2024-11-14T20:54:31.360699Z","shell.execute_reply":"2024-11-14T20:54:31.594234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:31.596138Z","iopub.execute_input":"2024-11-14T20:54:31.596494Z","iopub.status.idle":"2024-11-14T20:54:31.607841Z","shell.execute_reply.started":"2024-11-14T20:54:31.596459Z","shell.execute_reply":"2024-11-14T20:54:31.606864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nax = sns.countplot(data=tr_df, x=tr_df['Class'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:31.609368Z","iopub.execute_input":"2024-11-14T20:54:31.610049Z","iopub.status.idle":"2024-11-14T20:54:31.948223Z","shell.execute_reply.started":"2024-11-14T20:54:31.610000Z","shell.execute_reply":"2024-11-14T20:54:31.947147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.optimizers import Adamax\n\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:31.951869Z","iopub.execute_input":"2024-11-14T20:54:31.952166Z","iopub.status.idle":"2024-11-14T20:54:44.696725Z","shell.execute_reply.started":"2024-11-14T20:54:31.952135Z","shell.execute_reply":"2024-11-14T20:54:44.695861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_df, ts_df = train_test_split(ts_df, train_size=0.5, stratify=ts_df['Class'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:44.698012Z","iopub.execute_input":"2024-11-14T20:54:44.698715Z","iopub.status.idle":"2024-11-14T20:54:44.709393Z","shell.execute_reply.started":"2024-11-14T20:54:44.698671Z","shell.execute_reply":"2024-11-14T20:54:44.708285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\n\nimg_size = (299, 299)\n\nimage_generator = ImageDataGenerator(rescale=1/255, brightness_range=(0.8, 1.2))\n\nts_gen = ImageDataGenerator(rescale=1/255)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:44.710714Z","iopub.execute_input":"2024-11-14T20:54:44.711024Z","iopub.status.idle":"2024-11-14T20:54:44.749254Z","shell.execute_reply.started":"2024-11-14T20:54:44.710991Z","shell.execute_reply":"2024-11-14T20:54:44.748157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_gen = image_generator.flow_from_dataframe(tr_df, x_col='Class Path',\n                                             y_col='Class', batch_size=batch_size, target_size=img_size)\n\nvalid_gen = image_generator.flow_from_dataframe(valid_df, x_col='Class Path',\n                                                y_col='Class', batch_size=batch_size, target_size=img_size)\n\nts_gen = ts_gen.flow_from_dataframe(ts_df, x_col='Class Path',\n                                    y_col='Class', batch_size=16, target_size=img_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:44.750674Z","iopub.execute_input":"2024-11-14T20:54:44.751044Z","iopub.status.idle":"2024-11-14T20:54:49.765640Z","shell.execute_reply.started":"2024-11-14T20:54:44.751008Z","shell.execute_reply":"2024-11-14T20:54:49.764798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    batch = next(tr_gen)\n    image = batch[0][0]\n    label = batch[1][0]\n    plt.imshow(image)\n\n    # Get the class index\n    class_index = np.argmax(label)\n\n    # Get the list of class names and class indices\n    class_names = list(tr_gen.class_indices.keys())\n    class_indices = list(tr_gen.class_indices.values())\n\n    # Find the index of the class_index in the list of indices\n    index_position = class_indices.index(class_index)\n\n    # Get the class name using the index position\n    class_name = class_names[index_position]\n\n    plt.title(f\"Class: {class_name}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:49.766823Z","iopub.execute_input":"2024-11-14T20:54:49.767163Z","iopub.status.idle":"2024-11-14T20:54:56.664508Z","shell.execute_reply.started":"2024-11-14T20:54:49.767127Z","shell.execute_reply":"2024-11-14T20:54:56.663253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_shape = (299, 299, 3)\n\nbase_model = tf.keras.applications.Xception(include_top=False,\n                                          weights=\"imagenet\",\n                                          input_shape=img_shape,\n                                          pooling='max')\n\nmodel = Sequential([\n    base_model,\n    Flatten(),\n    Dropout(rate=0.3),\n    Dense(128, activation='relu'),\n    Dropout(rate=0.25),\n    Dense(4,\n activation='softmax')\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:54:56.665916Z","iopub.execute_input":"2024-11-14T20:54:56.666301Z","iopub.status.idle":"2024-11-14T20:55:01.677116Z","shell.execute_reply.started":"2024-11-14T20:54:56.666264Z","shell.execute_reply":"2024-11-14T20:55:01.676231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(Adamax(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy',\n                        Precision(),\n                        Recall()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:55:01.678515Z","iopub.execute_input":"2024-11-14T20:55:01.678919Z","iopub.status.idle":"2024-11-14T20:55:01.701626Z","shell.execute_reply.started":"2024-11-14T20:55:01.678868Z","shell.execute_reply":"2024-11-14T20:55:01.700830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:55:01.702856Z","iopub.execute_input":"2024-11-14T20:55:01.703277Z","iopub.status.idle":"2024-11-14T20:55:01.710614Z","shell.execute_reply.started":"2024-11-14T20:55:01.703231Z","shell.execute_reply":"2024-11-14T20:55:01.709547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hist = model.fit(tr_gen, epochs=5, validation_data=valid_gen)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T20:55:01.711879Z","iopub.execute_input":"2024-11-14T20:55:01.712212Z","iopub.status.idle":"2024-11-14T21:03:36.697855Z","shell.execute_reply.started":"2024-11-14T20:55:01.712167Z","shell.execute_reply":"2024-11-14T21:03:36.696960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get training and validation metrics from history\nmetrics = ['accuracy', 'loss', 'precision', 'recall']\ntr_metrics = {m: hist.history[m] for m in metrics}\nval_metrics = {m: hist.history[f'val_{m}'] for m in metrics}\n\n# Find best epochs and values\nbest_epochs = {}\nbest_values = {}\nfor m in metrics:\n    if m == 'loss':\n        idx = np.argmin(val_metrics[m])\n    else:\n        idx = np.argmax(val_metrics[m])\n    best_epochs[m] = idx + 1\n    best_values[m] = val_metrics[m][idx]\n\n# Plot metrics\nplt.figure(figsize=(20, 12))\nplt.style.use('fivethirtyeight')\n\nfor i, metric in enumerate(metrics, 1):\n    plt.subplot(2, 2, i)\n    epochs = range(1, len(tr_metrics[metric]) + 1)\n\n    plt.plot(epochs, tr_metrics[metric], 'r', label=f'Training ({metric})')\n    plt.plot(epochs, val_metrics[metric], 'g', label=f'Validation ({metric})')\n    plt.scatter(best_epochs[metric], best_values[metric], s=150, c='blue',\n                label=f'Best epoch ({best_epochs[metric]})')\n\n    plt.title(f'Training and Validation ({metric.title()})')\n    plt.xlabel('Epochs')\n    plt.ylabel(metric.title())\n    plt.legend()\n    plt.grid(True)\n\nplt.suptitle('Model Training Metrics Over Epochs', fontsize=16)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:03:53.126700Z","iopub.execute_input":"2024-11-14T21:03:53.127241Z","iopub.status.idle":"2024-11-14T21:03:54.421557Z","shell.execute_reply.started":"2024-11-14T21:03:53.127171Z","shell.execute_reply":"2024-11-14T21:03:54.420504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_score = model.evaluate(tr_gen, verbose=1)\nvalid_score = model.evaluate(valid_gen, verbose=1)\ntest_score = model.evaluate(ts_gen, verbose=1)\n\nprint(f\"Train Accuracy: {(train_score[1]*100):.2f}%\")\nprint(f\"Train Loss: {train_score[0]:.4f}\")\nprint(f\"\\n\\nValidation Accuracy: {(valid_score[1]*100):.2f}%\")\nprint(f\"Validation Loss: {valid_score[0]:.4f}\")\nprint(f\"\\n\\nTest Accuracy: {(test_score[1]*100):.2f}%\")\nprint(f\"Test Loss: {test_score[0]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:03:58.881067Z","iopub.execute_input":"2024-11-14T21:03:58.881934Z","iopub.status.idle":"2024-11-14T21:04:41.788879Z","shell.execute_reply.started":"2024-11-14T21:03:58.881874Z","shell.execute_reply":"2024-11-14T21:04:41.787906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = model.predict(ts_gen)\ny_pred = np.argmax(preds, axis=1)\n\nclass_dict = {\n    0: 'glioma',\n    1: 'meningioma',\n    2: 'no_tumor',\n    3: 'pituitary'\n}\n\n# Then create and display the confusion matrix\ncm = confusion_matrix(ts_gen.classes, y_pred)\n\nlabels = list(class_dict.keys())\nplt.figure(figsize=(10, 8))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:04:45.572229Z","iopub.execute_input":"2024-11-14T21:04:45.573207Z","iopub.status.idle":"2024-11-14T21:04:50.924287Z","shell.execute_reply.started":"2024-11-14T21:04:45.573152Z","shell.execute_reply":"2024-11-14T21:04:50.923332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\ndef predict(img_path: str) -> None:\n    # Get class labels\n    labels = list(class_dict.keys())\n\n    # Create figure\n    plt.figure(figsize=(6, 8))\n\n    # Load and preprocess image\n    img = Image.open(img_path)\n    resized_img = img.resize((299, 299))\n\n    img_array = np.asarray(resized_img)\n    img_array = np.expand_dims(img_array, axis=0) / 255.0\n\n    # Get model predictions\n    predictions = model.predict(img_array)\n    probabilities = list(predictions[0])\n\n    # Get predicted class\n    predicted_class_idx = np.argmax(probabilities)\n    predicted_class = class_dict[predicted_class_idx]\n\n    # Plot original image\n    plt.subplot(2, 1, 1)\n    plt.imshow(resized_img)\n    plt.title(f\"Input MRI Image\\nPredicted: {predicted_class}\")\n\n    # Plot prediction probabilities\n    plt.subplot(2, 1, 2)\n    bars = plt.barh(labels, probabilities)\n    plt.xlabel(\"Probability\", fontsize=15)\n    plt.title(\"Class Probabilities\")\n\n    # Add probability labels to bars\n    ax = plt.gca()\n    ax.bar_label(bars, fmt=\"%.2f\")\n\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"\\nPredicted tumor type: {predicted_class}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:04:56.395558Z","iopub.execute_input":"2024-11-14T21:04:56.396482Z","iopub.status.idle":"2024-11-14T21:04:56.405599Z","shell.execute_reply.started":"2024-11-14T21:04:56.396440Z","shell.execute_reply":"2024-11-14T21:04:56.404472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict(\"/kaggle/input/brain-tumor-mri-dataset/Testing/meningioma/Te-meTr_0000.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:05:36.071892Z","iopub.execute_input":"2024-11-14T21:05:36.072898Z","iopub.status.idle":"2024-11-14T21:05:39.879945Z","shell.execute_reply.started":"2024-11-14T21:05:36.072855Z","shell.execute_reply":"2024-11-14T21:05:39.874818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict(\"/kaggle/input/brain-tumor-mri-dataset/Testing/meningioma/Te-meTr_0005.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:06:10.349931Z","iopub.execute_input":"2024-11-14T21:06:10.350703Z","iopub.status.idle":"2024-11-14T21:06:10.894694Z","shell.execute_reply.started":"2024-11-14T21:06:10.350658Z","shell.execute_reply":"2024-11-14T21:06:10.893657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict(\"/kaggle/input/brain-tumor-mri-dataset/Testing/glioma/Te-glTr_0000.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:06:55.738360Z","iopub.execute_input":"2024-11-14T21:06:55.739378Z","iopub.status.idle":"2024-11-14T21:06:56.283940Z","shell.execute_reply.started":"2024-11-14T21:06:55.739332Z","shell.execute_reply":"2024-11-14T21:06:56.282923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_weights('/kaggle/content/xception_model.weights.h5')\nmodel.save_weights('/kaggle/working/xception_model.weights.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:10:10.355348Z","iopub.execute_input":"2024-11-14T21:10:10.356231Z","iopub.status.idle":"2024-11-14T21:10:12.199277Z","shell.execute_reply.started":"2024-11-14T21:10:10.356168Z","shell.execute_reply":"2024-11-14T21:10:12.198399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import regularizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:10:36.153874Z","iopub.execute_input":"2024-11-14T21:10:36.154818Z","iopub.status.idle":"2024-11-14T21:10:36.159770Z","shell.execute_reply.started":"2024-11-14T21:10:36.154771Z","shell.execute_reply":"2024-11-14T21:10:36.158486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 16\nimg_size = (224, 224)\n\nimage_generator = ImageDataGenerator(rescale=1/255, brightness_range=(0.8, 1.2))\n\nts_gen = ImageDataGenerator(rescale=1/255)\n\n\ntr_gen = image_generator.flow_from_dataframe(tr_df, x_col='Class Path',\n                                             y_col='Class', batch_size=batch_size, target_size=img_size)\nvalid_gen = image_generator.flow_from_dataframe(valid_df, x_col='Class Path',\n                                                y_col='Class', batch_size=batch_size, target_size=img_size)\nts_gen = ts_gen.flow_from_dataframe(ts_df, x_col='Class Path',\n                                    y_col='Class', batch_size=16, target_size=img_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:10:39.404722Z","iopub.execute_input":"2024-11-14T21:10:39.405457Z","iopub.status.idle":"2024-11-14T21:10:42.602701Z","shell.execute_reply.started":"2024-11-14T21:10:39.405405Z","shell.execute_reply":"2024-11-14T21:10:42.601714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Sequential model\ncnn_model = Sequential()\n\n# Convolutional layers\ncnn_model.add(Conv2D(512, (3, 3), padding='same', input_shape=(224, 224, 3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n\ncnn_model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n\ncnn_model.add(Dropout(0.25))\n\n\ncnn_model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n\ncnn_model.add(Dropout(0.25))\n\n\ncnn_model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten the output for fully connected layers\ncnn_model.add(Flatten())\n\n# Fully connected layers\ncnn_model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n\ncnn_model.add(Dropout(0.35))\n\ncnn_model.add(Dense(4, activation='softmax'))  # Output layer with 4 neurons for the 4 classes\n\n# Compile the model\ncnn_model.compile(Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n\n# Display the model summary\ncnn_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:11:03.896638Z","iopub.execute_input":"2024-11-14T21:11:03.897343Z","iopub.status.idle":"2024-11-14T21:11:04.096325Z","shell.execute_reply.started":"2024-11-14T21:11:03.897299Z","shell.execute_reply":"2024-11-14T21:11:04.095254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = cnn_model.fit(tr_gen, epochs=20, validation_data=valid_gen)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:11:09.225121Z","iopub.execute_input":"2024-11-14T21:11:09.225561Z","iopub.status.idle":"2024-11-14T21:27:37.498372Z","shell.execute_reply.started":"2024-11-14T21:11:09.225523Z","shell.execute_reply":"2024-11-14T21:27:37.497425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install groq\nfrom groq import Groq\n#from google.colab import userdata\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\ngroq_client = Groq(api_key=user_secrets.get_secret(\"GROQ_API_KEY\"))\n\nprompt = f\"\"\"You are an expert Data Scientist specializing in medical image analysis,\ndeep learning architectures, and computer vision tasks,\nwith particular expertise in MRI scan classification.\nGiven the current CNN model architecture and its training history for brain tumor classification,\nprovide a comprehensive analysis and architectural improvements.\n\nContext:\n\n-Task: Multi-class classification of brain tumors from MRI scans\n-Classes: 4 (specific tumor types)\n-Input dimensions: 224x224x3 RGB images\n-Current validation accuracy: 0.8994\n-Current validation precision: 0.9068\n-Current validation recall: 0.8902\n\nCurrent Architecture Analysis:\n<CNN MODEL ARCHITECTURE>\n\n# Create a Sequential model\ncnn_model = Sequential()\n\n# Convolutional layers\ncnn_model.add(Conv2D(512, (3, 3), padding='same', input_shape=(224, 224, 3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n\ncnn_model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n\ncnn_model.add(Dropout(0.25))\n\n\ncnn_model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n\ncnn_model.add(Dropout(0.25))\n\n\ncnn_model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flatten the output for fully connected layers\ncnn_model.add(Flatten())\n\n# Fully connected layers\ncnn_model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n\ncnn_model.add(Dropout(0.35))\n\ncnn_model.add(Dense(4, activation='softmax'))  # Output layer with 4 neurons for the 4 classes\n\n# Compile the model\ncnn_model.compile(Adamax(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n\n# Display the model summary\ncnn_model.summary()\n\n</CNN_MODEL_ARCHITECTURE>\n\nTraining History Observations:\n\n<TRAINING_HISTORY>\n\nEpoch 1/5\n357/357 ━━━━━━━━━━━━━━━━━━━━ 136s 297ms/step - accuracy: 0.5612 - loss: 2.3783 - precision_2: 0.6691 - recall_2: 0.3793 - val_accuracy: 0.7439 - val_loss: 0.9620 - val_precision_2: 0.7855 - val_recall_2: 0.6585\nEpoch 2/5\n357/357 ━━━━━━━━━━━━━━━━━━━━ 111s 287ms/step - accuracy: 0.8002 - loss: 0.7874 - precision_2: 0.8281 - recall_2: 0.7647 - val_accuracy: 0.8201 - val_loss: 0.7152 - val_precision_2: 0.8525 - val_recall_2: 0.7927\nEpoch 3/5\n357/357 ━━━━━━━━━━━━━━━━━━━━ 103s 287ms/step - accuracy: 0.8467 - loss: 0.5812 - precision_2: 0.8651 - recall_2: 0.8362 - val_accuracy: 0.7591 - val_loss: 0.6902 - val_precision_2: 0.7714 - val_recall_2: 0.7409\nEpoch 4/5\n357/357 ━━━━━━━━━━━━━━━━━━━━ 103s 288ms/step - accuracy: 0.8761 - loss: 0.4702 - precision_2: 0.8862 - recall_2: 0.8646 - val_accuracy: 0.8994 - val_loss: 0.4464 - val_precision_2: 0.8985 - val_recall_2: 0.8902\nEpoch 5/5\n357/357 ━━━━━━━━━━━━━━━━━━━━ 103s 287ms/step - accuracy: 0.9129 - loss: 0.3708 - precision_2: 0.9185 - recall_2: 0.9048 - val_accuracy: 0.8994 - val_loss: 0.4238 - val_precision_2: 0.9068 - val_recall_2: 0.8902\n\n</TRAINING_HISTORY>\n\nPlease provide detailed recommendations for architectural improvements, addressing:\n\nNetwork Depth and Width:\n\n-Optimal number of convolutional layers\n-Filter sizes and counts progression\n-Potential for residual connections\n\nRegularization Strategy:\n\n-Dropout rates and placement\n-Batch normalization implementation\n-L1/L2 regularization adjustments\n\nFeature Extraction:\n\n-Receptive field optimization\n-Pooling strategies\n-Advanced convolution techniques (dilated, separable)\n\nLearning Process:\n\n-Learning rate schedules\n-Batch size considerations\n-Optimizer recommendations\n\nModern Architecture Components:\n\n-Potential for attention mechanisms\n-Skip connections\n-Feature pyramid networks\n\nMedical Imaging Specific:\n\n-Techniques for handling MRI characteristics\n-Strategies for class imbalance\n-Data augmentation specific to medical imaging\n\nPlease provide:\n\n-Complete revised architecture with detailed explanations\n-Expected impact on metrics\n-Computational considerations\n-Implementation recommendations\n\nInclude code snippets in TensorFlow/Keras format with detailed comments explaining each architectural decision.\nLet's think step by step about this. Verify step by step.\n \"\"\"\n\nresponse = groq_client.chat.completions.create(\n    model=\"llama-3.1-70b-versatile\", # model=\"llama-3.1-8b-instant\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": prompt\n\n         }\n    ]\n)\n\nprint(response.choices[0].message.content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:28:00.920375Z","iopub.execute_input":"2024-11-14T21:28:00.921416Z","iopub.status.idle":"2024-11-14T21:28:24.649416Z","shell.execute_reply.started":"2024-11-14T21:28:00.921374Z","shell.execute_reply":"2024-11-14T21:28:24.648206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get training and validation metrics from history\n#metrics = ['accuracy', 'loss', 'precision', 'recall']\nmetrics = ['accuracy', 'loss', 'precision_1', 'recall_1']\n\ntr_metrics = {m: history.history[m] for m in metrics}\nval_metrics = {m: history.history[f'val_{m}'] for m in metrics}\n\n# Find best epochs and values\nbest_epochs = {}\nbest_values = {}\nfor m in metrics:\n    if m == 'loss':\n        idx = np.argmin(val_metrics[m])\n    else:\n        idx = np.argmax(val_metrics[m])\n    best_epochs[m] = idx + 1\n    best_values[m] = val_metrics[m][idx]\n\n# Plot metrics\nplt.figure(figsize=(20, 12))\nplt.style.use('fivethirtyeight')\n\nfor i, metric in enumerate(metrics, 1):\n    plt.subplot(2, 2, i)\n    epochs = range(1, len(tr_metrics[metric]) + 1)\n\n    plt.plot(epochs, tr_metrics[metric], 'r', label=f'Training ({metric})')\n    plt.plot(epochs, val_metrics[metric], 'g', label=f'Validation ({metric})')\n    plt.scatter(best_epochs[metric], best_values[metric], s=150, c='blue',\n                label=f'Best epoch ({best_epochs[metric]})')\n\n    plt.title(f'Training and Validation ({metric.title()})')\n    plt.xlabel('Epochs')\n    plt.ylabel(metric.title())\n    plt.legend()\n    plt.grid(True)\n\nplt.suptitle('Model Training Metrics Over Epochs', fontsize=16)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:28:33.754674Z","iopub.execute_input":"2024-11-14T21:28:33.755080Z","iopub.status.idle":"2024-11-14T21:28:35.017914Z","shell.execute_reply.started":"2024-11-14T21:28:33.755043Z","shell.execute_reply":"2024-11-14T21:28:35.016901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_score = cnn_model.evaluate(tr_gen, verbose=1)\nvalid_score = cnn_model.evaluate(valid_gen, verbose=1)\ntest_score = cnn_model.evaluate(ts_gen, verbose=1)\n\nprint(f\"Train Accuracy: {(train_score[1]*100):.2f}%\")\nprint(f\"Train Loss: {train_score[0]:.4f}\")\nprint(f\"\\n\\nValidation Accuracy: {(valid_score[1]*100):.2f}%\")\nprint(f\"Validation Loss: {valid_score[0]:.4f}\")\nprint(f\"\\n\\nTest Accuracy: {(test_score[1]*100):.2f}%\")\nprint(f\"Test Loss: {test_score[0]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:28:39.396289Z","iopub.execute_input":"2024-11-14T21:28:39.396727Z","iopub.status.idle":"2024-11-14T21:29:06.303015Z","shell.execute_reply.started":"2024-11-14T21:28:39.396686Z","shell.execute_reply":"2024-11-14T21:29:06.301875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = cnn_model.predict(ts_gen)\ny_pred = np.argmax(preds, axis=1)\n\nclass_dict = {\n    0: 'glioma',\n    1: 'meningioma',\n    2: 'no_tumor',\n    3: 'pituitary'\n}\n\n# Then create and display the confusion matrix\ncm = confusion_matrix(ts_gen.classes, y_pred)\n\nlabels = list(class_dict.keys())\nplt.figure(figsize=(10, 8))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:29:11.050519Z","iopub.execute_input":"2024-11-14T21:29:11.051121Z","iopub.status.idle":"2024-11-14T21:29:13.321775Z","shell.execute_reply.started":"2024-11-14T21:29:11.051067Z","shell.execute_reply":"2024-11-14T21:29:13.320720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clr = classification_report(ts_gen.classes, y_pred)\nprint(clr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:29:13.323406Z","iopub.execute_input":"2024-11-14T21:29:13.324241Z","iopub.status.idle":"2024-11-14T21:29:13.342261Z","shell.execute_reply.started":"2024-11-14T21:29:13.324171Z","shell.execute_reply":"2024-11-14T21:29:13.341151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnn_model.save(\"/kaggle/content/cnn_model.h5\")\ncnn_model.save(\"/kaggle/working/cnn_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:30:23.699787Z","iopub.execute_input":"2024-11-14T21:30:23.700220Z","iopub.status.idle":"2024-11-14T21:30:24.031850Z","shell.execute_reply.started":"2024-11-14T21:30:23.700169Z","shell.execute_reply":"2024-11-14T21:30:24.030820Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## InceptionV3","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\nimg_size = (299, 299)\n\nimage_generator = ImageDataGenerator(rescale=1/255, brightness_range=(0.8, 1.2))\n\nts_gen = ImageDataGenerator(rescale=1/255)\n\ntr_gen = image_generator.flow_from_dataframe(tr_df, x_col='Class Path',\n                                             y_col='Class', batch_size=batch_size, target_size=img_size)\n\nvalid_gen = image_generator.flow_from_dataframe(valid_df, x_col='Class Path',\n                                                y_col='Class', batch_size=batch_size, target_size=img_size)\n\nts_gen = ts_gen.flow_from_dataframe(ts_df, x_col='Class Path',\n                                    y_col='Class', batch_size=16, target_size=img_size, shuffle=False)\n\n\n\n\nimg_shape = (299, 299, 3)\n\nbase__model = tf.keras.applications.InceptionV3(include_top=False,\n                                          weights=\"imagenet\",\n                                          input_shape=img_shape,\n                                          pooling='max')\n\nmodel_tf = Sequential([\n    base__model,\n    Flatten(),\n    Dropout(rate=0.3),\n    Dense(128, activation='relu'),\n    Dropout(rate=0.25),\n    Dense(4,\n activation='softmax')\n])\n\nmodel_tf.compile(Adamax(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy',\n                        Precision(),\n                        Recall()])\n\nhist_tf = model_tf.fit(tr_gen, epochs=8, validation_data=valid_gen)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:30:31.609230Z","iopub.execute_input":"2024-11-14T21:30:31.610266Z","iopub.status.idle":"2024-11-14T21:40:38.559001Z","shell.execute_reply.started":"2024-11-14T21:30:31.610205Z","shell.execute_reply":"2024-11-14T21:40:38.558063Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2: Streamlit Web App","metadata":{}},{"cell_type":"code","source":"!pip install google-generativeai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T22:25:34.854657Z","iopub.execute_input":"2024-11-14T22:25:34.855120Z","iopub.status.idle":"2024-11-14T22:25:47.180514Z","shell.execute_reply.started":"2024-11-14T22:25:34.855081Z","shell.execute_reply":"2024-11-14T22:25:47.179258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install streamlit pyngrok python-dotenv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T22:25:49.964327Z","iopub.execute_input":"2024-11-14T22:25:49.964751Z","iopub.status.idle":"2024-11-14T22:26:02.084069Z","shell.execute_reply.started":"2024-11-14T22:25:49.964714Z","shell.execute_reply":"2024-11-14T22:26:02.082729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n%%writefile /kaggle/content/.env\nGOOGLE_API_KEY=your_api_key\nGROQ_API_KEY=your_api_key\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T22:26:02.096140Z","iopub.execute_input":"2024-11-14T22:26:02.096512Z","iopub.status.idle":"2024-11-14T22:26:02.114765Z","shell.execute_reply.started":"2024-11-14T22:26:02.096473Z","shell.execute_reply":"2024-11-14T22:26:02.113718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/content/.env\nGOOGLE_API_KEY=AIzaSyBQP_bjm6karXRTnmU2AbQAT5pv4Vn8LNg\nGROQ_API_KEY=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","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T22:26:02.116753Z","iopub.execute_input":"2024-11-14T22:26:02.117296Z","iopub.status.idle":"2024-11-14T22:26:02.125106Z","shell.execute_reply.started":"2024-11-14T22:26:02.117248Z","shell.execute_reply":"2024-11-14T22:26:02.124169Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/.env\nGOOGLE_API_KEY=AIzaSyBQP_bjm6karXRTnmU2AbQAT5pv4Vn8LNg\nGROQ_API_KEY=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","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-11-14T22:26:07.435765Z","iopub.execute_input":"2024-11-14T22:26:07.436235Z","iopub.status.idle":"2024-11-14T22:26:07.446240Z","shell.execute_reply.started":"2024-11-14T22:26:07.436188Z","shell.execute_reply":"2024-11-14T22:26:07.445010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from threading import Thread\nfrom pyngrok import ngrok\n#from google.colab import userdata #works in google colab only, \n#you can use in %%writefile /kaggle/working/app.py\n#genai.configure(api_key=userdata.get('GOOGLE_API_KEY')) #works in google colab only,\n#you can use in %%writefile /kaggle/working/app.py\n#client = Groq(api_key=userdata.get('GROQ_API_KEY'))#works in google colab only,\n#you can use in %%writefile /kaggle/working/app.py\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T22:26:10.125635Z","iopub.execute_input":"2024-11-14T22:26:10.126050Z","iopub.status.idle":"2024-11-14T22:26:10.131510Z","shell.execute_reply.started":"2024-11-14T22:26:10.126013Z","shell.execute_reply":"2024-11-14T22:26:10.130572Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ngrok_token = user_secrets.get_secret(\"NGROK_AUTH_TOKEN\")\n\nngrok.set_auth_token(ngrok_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T22:26:12.593661Z","iopub.execute_input":"2024-11-14T22:26:12.594411Z","iopub.status.idle":"2024-11-14T22:26:12.819637Z","shell.execute_reply.started":"2024-11-14T22:26:12.594368Z","shell.execute_reply":"2024-11-14T22:26:12.818432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_streamlit():\n    import os # Import the os module\n    os.system(\"streamlit run /kaggle/working/app.py --server.port 8501\")\n    #os.system(\"streamlit run /kaggle/content/app.py --server.port 8501\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T22:45:13.978133Z","iopub.execute_input":"2024-11-14T22:45:13.979025Z","iopub.status.idle":"2024-11-14T22:45:13.983459Z","shell.execute_reply.started":"2024-11-14T22:45:13.978984Z","shell.execute_reply":"2024-11-14T22:45:13.982449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/app.py\nimport streamlit as st\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport plotly.graph_objects as go\nimport cv2\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.optimizers import Adamax\nfrom tensorflow.keras.metrics import Precision, Recall\nimport google.generativeai as genai\n\nimport PIL.Image\nimport os\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\ngenai.configure(api_key=os.getenv('GOOGLE_API_KEY'))\n\n\n#output_dir = 'saliency_maps'\n#os.makedirs(output_dir, exist_ok=True)\n\ndef generate_explanation(img_path, model_prediction, confidence):\n    prompt = f\"\"\"You are an expert neurologist. You are tasked with explaining a saliency map of a brain tumor MRI scan.\n    The saliency map was generated by a deep learning model that was trained to classify brain tumors\n    as either glioma, meningioma, pituitary, or no tumor.\n\n    The saliency map highlights the regions of the image that the machine learning model is focusing on to make the prediction.\n\n    The deep learning model predicted the image to be of class '{model_prediction}' with a confidence of {confidence * 100}%.\n\n    In your response:\n    - Explain what regions of the brain the model is focusing on, based on the saliency map. Refer to the regions highlighted\n    in light cyan, those are the regions where the model is focusing on.\n    - Explain possible reasons why the model made the prediction it did.\n    - Don't mention anything like 'The saliency map highlights the regions the model is focusing on, which are in light cyan'\n    in your explanation.\n    - Keep your explanation to 4 sentences max.\n\n    Let's think step by step about this. Verify step by step.\n    \"\"\"\n\n    img = PIL.Image.open(img_path)\n    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n    response = model.generate_content([prompt, img])\n\n    return response.text\n\n\ndef generate_explanation_groq(img_path, model_prediction, confidence):\n    prompt = f\"\"\"You are an expert neurologist. You are tasked with explaining a saliency map of a brain tumor MRI scan.\n    The saliency map was generated by a deep learning model that was trained to classify brain tumors\n    as either glioma, meningioma, pituitary, or no tumor.\n\n    The saliency map highlights the regions of the image that the machine learning model is focusing on to make the prediction.\n\n    The deep learning model predicted the image to be of class '{model_prediction}' with a confidence of {confidence * 100}%.\n\n    In your response:\n    - Explain what regions of the brain the model is focusing on, based on the saliency map. Refer to the regions highlighted\n    in light cyan, those are the regions where the model is focusing on.\n    - Explain possible reasons why the model made the prediction it did.\n    - Don't mention anything like 'The saliency map highlights the regions the model is focusing on, which are in light cyan'\n    in your explanation.\n    - Keep your explanation to 4 sentences max.\n\n    Let's think step by step about this. Verify step by step.\n    \"\"\"\n\n    from groq import Groq\n    import base64\n\n\n    # Function to encode the image\n    def encode_image(img_path):\n       with open(img_path, \"rb\") as image_file:\n          return base64.b64encode(image_file.read()).decode('utf-8')\n\n    # Getting the base64 string\n    base64_image = encode_image(img_path)\n\n    client = Groq(api_key=os.getenv('GROQ_API_KEY'))\n    #client = Groq(api_key=userdata.get('GROQ_API_KEY'))\n\n    chat_completion = client.chat.completions.create(\n      messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                    },\n                },\n            ],\n        }\n    ],\n    model=\"llama-3.2-11b-vision-preview\",\n    )\n\n\n    return chat_completion.choices[0].message.content\n\n\n\noutput_dir = 'saliency_maps'\nos.makedirs(output_dir, exist_ok=True)\n\ndef generate_saliency_map(model, img_array, class_index, img_size):\n    with tf.GradientTape() as tape:\n        img_tensor = tf.convert_to_tensor(img_array)\n        tape.watch(img_tensor)\n        predictions = model(img_tensor)\n        target_class = predictions[:, class_index]\n\n    gradients = tape.gradient(target_class, img_tensor)\n    gradients = tf.math.abs(gradients)\n    gradients = tf.reduce_max(gradients, axis=-1)\n    gradients = gradients.numpy().squeeze()\n\n    # Resize gradients to match original image size\n    gradients = cv2.resize(gradients, img_size)\n\n    # Create a circular mask for the brain area\n    center = (gradients.shape[0] // 2, gradients.shape[1] // 2)\n    radius = min(center[0], center[1]) // 10\n    y, x = np.ogrid[:gradients.shape[0], :gradients.shape[1]]\n    mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2\n\n    # Apply mask to gradients\n    gradients = gradients * mask\n\n    # Normalize only the brain area\n    brain_gradients = gradients[mask]\n    if brain_gradients.max() > brain_gradients.min():\n        brain_gradients = (brain_gradients - brain_gradients.min()) / (brain_gradients.max() - brain_gradients.min())\n    gradients[mask] = brain_gradients\n\n    # Apply a higher threshold\n    threshold = np.percentile(gradients[mask], 80)\n    gradients[gradients < threshold] = 0\n\n    # Apply more aggressive smoothing\n    gradients = cv2.GaussianBlur(gradients, (11, 11), 0)\n\n    # Create a heatmap overlay with enhanced contrast\n    heatmap = cv2.applyColorMap(np.uint8(255 * gradients), cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n\n    # Resize heatmap to match original image size\n    heatmap = cv2.resize(heatmap, img_size)\n\n    # Superimpose the heatmap on original image with increased opacity\n    original_img = image.img_to_array(img)\n    superimposed_img = heatmap * 0.7 + original_img * 0.3\n    superimposed_img = superimposed_img.astype(np.uint8)\n\n    img_path = os.path.join(output_dir, uploaded_file.name)\n    with open(img_path, \"wb\") as f:\n        f.write(uploaded_file.getbuffer())\n\n    saliency_map_path = f'saliency_maps/{uploaded_file.name}'\n\n    # Save the saliency map\n    cv2.imwrite(saliency_map_path, cv2.cvtColor(superimposed_img, cv2.COLOR_RGB2BGR))\n\n    return superimposed_img\n\n\ndef load_xception_model(model_path):\n    img_shape = (299, 299, 3)\n\n    base_model = tf.keras.applications.Xception(include_top=False, weights=\"imagenet\",\n                                             input_shape=img_shape, pooling='max')\n\n    model = Sequential([\n        base_model,\n        Flatten(),\n        Dropout(rate=0.3),\n        Dense(128, activation='relu'),\n        Dropout(rate=0.25),\n        Dense(4, activation='softmax')\n    ])\n\n    model.build((None,) + img_shape)\n\n    # Compile the model\n    model.compile(Adamax(learning_rate=0.001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy',\n                            Precision(),\n                            Recall()])\n\n    model.load_weights(model_path)\n\n    return model\n\n\n\n\nst.title(\"Brain Tumor Classification\")\n\nst.write(\"Upload an image of a brain MRI scan to classify.\")\n\nuploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n\nif uploaded_file is not None:\n\n    selected_model = st.radio(\n        \"Select Model\",\n        (\"Transfer Learning - Xception\", \"Custom CNN\")\n    )\n\n    if selected_model == \"Transfer Learning - Xception\":\n        model = load_xception_model('/kaggle/working/xception_model.weights.h5')\n        #model = load_xception_model('/kaggle/content/xception_model.weights.h5')\n        img_size = (299, 299)\n    else:\n        model = load_model('/kaggle/working/cnn_model.h5')\n        #model = load_model('/kaggle/content/cnn_model.h5')\n        img_size = (224, 224)\n\n    labels = ['Glioma', 'Meningioma', 'No tumor', 'Pituitary']\n    img = image.load_img(uploaded_file, target_size=img_size)\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array /= 255.0\n\n    prediction = model.predict(img_array)\n\n\n    # Get the class with the highest probability\n    class_index = np.argmax(prediction[0])\n    result = labels[class_index]\n\n    st.write(f\"Predicted Class: {result}\")\n    st.write(\"Predictions:\")\n    for label, prob in zip(labels, prediction[0]):\n        st.write(f\"{label}: {prob:.4f}\")\n\n    saliency_map = generate_saliency_map(model, img_array, class_index, img_size)\n\n    col1, col2 = st.columns(2)\n    with col1:\n      st.image(uploaded_file, caption=\"Uploaded Image\", use_column_width=True)\n    with col2:\n      st.image(saliency_map, caption=\"Saliency Map\", use_column_width=True)\n\n\n    st.write(\"## Classification Results\")\n\n    result_container = st.container()\n    result_container = st.container()\n\n    result_container.markdown(\n    f\"\"\"\n    <div style=\"background-color: #000000; color: #ffffff; padding: 30px; border-radius: 15px;\">\n        <div style=\"display: flex; justify-content: space-between; align-items: center;\">\n            <div style=\"flex: 1; text-align: center;\">\n                <h3 style=\"color: #ffffff; margin-bottom: 10px; font-size: 20px;\">Prediction</h3>\n                <p style=\"font-size: 36px; font-weight: 800; color: #FF0000; margin: 0;\">\n                    {result}\n                </p>\n            </div>\n            <div style=\"width: 2px; height: 80px; background-color: #ffffff; margin: 0 20px;\"></div>\n            <div style=\"flex: 1; text-align: center;\">\n                <h3 style=\"color: #ffffff; margin-bottom: 10px; font-size: 20px;\">Confidence</h3>\n                <p style=\"font-size: 36px; font-weight: 800; color: #2196F3; margin: 0;\">\n                    {prediction[0][class_index]:.4%}\n                </p>\n            </div>\n        </div>\n    </div>\n    \"\"\",\n    unsafe_allow_html=True\n    )\n\n    # Prepare data for Plotly chart\n    probabilities = prediction[0]\n    sorted_indices = np.argsort(probabilities)[::-1]\n    sorted_labels = [labels[i] for i in sorted_indices]\n    sorted_probabilities = probabilities[sorted_indices]\n\n    # Create a Plotly bar chart\n    fig = go.Figure(go.Bar(\n    x=sorted_probabilities,\n    y=sorted_labels,\n    orientation='h',\n    marker_color=['red' if label == result else 'blue' for label in sorted_labels]\n    ))\n\n    # Customize the chart layout\n    fig.update_layout(\n    title='Probabilities for each class',\n    xaxis_title='Probability',\n    yaxis_title='Class',\n    height=400,\n    width=600,\n    yaxis=dict(autorange=\"reversed\")\n    )\n\n    # Add value labels to the bars\n    for i, prob in enumerate(sorted_probabilities):\n      fig.add_annotation(\n        x=prob,\n        y=i,\n        text=f'{prob:.4f}',\n        showarrow=False,\n        xshift=5\n        )\n\n    # Display the Plotly chart\n    st.plotly_chart(fig)\n\n    #Wihout chosing the llms models\n    #saliency_map_path = f'saliency_maps/{uploaded_file.name}'\n    #explanation = generate_explanation(saliency_map_path, result, prediction[0][class_index])\n\n    #st.write(\"# Explanation\")\n    #st.write(explanation)\n\n    selected_llm = st.radio(\n        \"Select LLM\",\n        (\"gemini-1.5-flash\", \"llama-3.2-11b-vision-preview\")\n    )\n\n    if selected_llm == \"gemini-1.5-flash\":\n        saliency_map_path = f'saliency_maps/{uploaded_file.name}'\n        explanation = generate_explanation(saliency_map_path, result, prediction[0][class_index])\n\n        st.write(\"# Explanation\")\n        st.write(explanation)\n\n    else:\n        saliency_map_path = f'saliency_maps/{uploaded_file.name}'\n        explanation = generate_explanation_groq(saliency_map_path, result, prediction[0][class_index])\n\n        st.write(\"# Explanation\")\n        st.write(explanation)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"thread = Thread(target=run_streamlit)\nthread.start()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Disconnect all existing tunnels\nfor tunnel in ngrok.get_tunnels():\n    ngrok.disconnect(tunnel.public_url)\n\n# Now connect your new tunnel\npublic_url = ngrok.connect(addr='8501', proto='http', bind_tls=True)\n\nprint(\"Public URL:\", public_url)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ntunnels = ngrok.get_tunnels()\nfor tunnel in tunnels:\n    print(f\"Closing tunnel: {tunnel.public_url} -> {tunnel.config['addr']}\")\n    ngrok.disconnect(tunnel.public_url)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T21:03:37.518380Z","iopub.status.idle":"2024-11-14T21:03:37.518836Z","shell.execute_reply.started":"2024-11-14T21:03:37.518566Z","shell.execute_reply":"2024-11-14T21:03:37.518593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}